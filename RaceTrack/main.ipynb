{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Box([-1.  0.  0.], 1.0, (3,), float32)\n",
      "Observation space: Box(0, 255, (96, 96, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "environment_name = 'CarRacing-v2'\n",
    "env = gym.make(environment_name, render_mode=\"human\")\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# Action and observation spaces\n",
    "print('Action space:', env.action_space)\n",
    "print('Observation space:', env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 5\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    score = 0\n",
    "\n",
    "    while not terminated and not truncated:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(environment_name, render_mode=\"human\")\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "\n",
    "env = make_vec_env('CarRacing-v0', n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "model = PPO('CnnPolicy', env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1167..1467 -> 300-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1096..1374 -> 278-tiles track\n",
      "Logging to Training\\Logs\\PPO_10\n",
      "Track generation: 1069..1343 -> 274-tiles track\n",
      "retry to generate track (normal if there are not manyinstances of this message)\n",
      "Track generation: 1104..1384 -> 280-tiles track\n",
      "Track generation: 1089..1365 -> 276-tiles track\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -51.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 69       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Track generation: 1183..1483 -> 300-tiles track\n",
      "Track generation: 1155..1456 -> 301-tiles track\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | -56.7      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 37         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 108        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02430618 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -4         |\n",
      "|    explained_variance   | 0.16       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00328   |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0287    |\n",
      "|    std                  | 0.913      |\n",
      "|    value_loss           | 0.243      |\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_path = os.path.join('Training', 'Saved Models', 'PPO_Driving_100k')\n",
    "model.save(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x21fd37188b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_path = os.path.join('Training', 'Saved Models', 'PPO_Driving_1M')\n",
    "model.load(ppo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1111..1398 -> 287-tiles track\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Track generation: 1172..1469 -> 297-tiles track\n",
      "Track generation: 1032..1291 -> 259-tiles track\n",
      "Track generation: 1216..1524 -> 308-tiles track\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m evaluate_policy(model, env, n_eval_episodes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, render\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:87\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[39mwhile\u001b[39;00m (episode_counts \u001b[39m<\u001b[39m episode_count_targets)\u001b[39m.\u001b[39many():\n\u001b[0;32m     86\u001b[0m     actions, states \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(observations, state\u001b[39m=\u001b[39mstates, episode_start\u001b[39m=\u001b[39mepisode_starts, deterministic\u001b[39m=\u001b[39mdeterministic)\n\u001b[1;32m---> 87\u001b[0m     observations, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m     88\u001b[0m     current_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[0;32m     89\u001b[0m     current_lengths \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py:377\u001b[0m, in \u001b[0;36mCarRacing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworld\u001b[39m.\u001b[39mStep(\u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m FPS, \u001b[39m6\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m, \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m \u001b[39m30\u001b[39m)\n\u001b[0;32m    375\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m FPS\n\u001b[1;32m--> 377\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender(\u001b[39m\"\u001b[39;49m\u001b[39mstate_pixels\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    379\u001b[0m step_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    380\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py:460\u001b[0m, in \u001b[0;36mCarRacing.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    458\u001b[0m gl\u001b[39m.\u001b[39mglViewport(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, VP_W, VP_H)\n\u001b[0;32m    459\u001b[0m t\u001b[39m.\u001b[39menable()\n\u001b[1;32m--> 460\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrender_road()\n\u001b[0;32m    461\u001b[0m \u001b[39mfor\u001b[39;00m geom \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mviewer\u001b[39m.\u001b[39monetime_geoms:\n\u001b[0;32m    462\u001b[0m     geom\u001b[39m.\u001b[39mrender()\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\gym\\envs\\box2d\\car_racing.py:531\u001b[0m, in \u001b[0;36mCarRacing.render_road\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    526\u001b[0m         polygons_\u001b[39m.\u001b[39mextend([p[\u001b[39m0\u001b[39m], p[\u001b[39m1\u001b[39m], \u001b[39m0\u001b[39m])\n\u001b[0;32m    528\u001b[0m vl \u001b[39m=\u001b[39m pyglet\u001b[39m.\u001b[39mgraphics\u001b[39m.\u001b[39mvertex_list(\n\u001b[0;32m    529\u001b[0m     \u001b[39mlen\u001b[39m(polygons_) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m, (\u001b[39m\"\u001b[39m\u001b[39mv3f\u001b[39m\u001b[39m\"\u001b[39m, polygons_), (\u001b[39m\"\u001b[39m\u001b[39mc4f\u001b[39m\u001b[39m\"\u001b[39m, colors)\n\u001b[0;32m    530\u001b[0m )  \u001b[39m# gl.GL_QUADS,\u001b[39;00m\n\u001b[1;32m--> 531\u001b[0m vl\u001b[39m.\u001b[39;49mdraw(gl\u001b[39m.\u001b[39;49mGL_QUADS)\n\u001b[0;32m    532\u001b[0m vl\u001b[39m.\u001b[39mdelete()\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\pyglet\\graphics\\vertexdomain.py:375\u001b[0m, in \u001b[0;36mVertexList.draw\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw\u001b[39m(\u001b[39mself\u001b[39m, mode):\n\u001b[0;32m    368\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Draw this vertex list in the given OpenGL mode.\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \n\u001b[0;32m    370\u001b[0m \u001b[39m    :Parameters:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \n\u001b[0;32m    374\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdomain\u001b[39m.\u001b[39;49mdraw(mode, \u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\duppu\\miniconda3\\envs\\introtoai\\lib\\site-packages\\pyglet\\graphics\\vertexdomain.py:290\u001b[0m, in \u001b[0;36mVertexDomain.draw\u001b[1;34m(self, mode, vertex_list)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw\u001b[39m(\u001b[39mself\u001b[39m, mode, vertex_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    275\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Draw vertices in the domain.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \n\u001b[0;32m    277\u001b[0m \u001b[39m    If `vertex_list` is not specified, all vertices in the domain are\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    288\u001b[0m \n\u001b[0;32m    289\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m     glPushClientAttrib(GL_CLIENT_VERTEX_ARRAY_BIT)\n\u001b[0;32m    291\u001b[0m     \u001b[39mfor\u001b[39;00m buffer, attributes \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_attributes:\n\u001b[0;32m    292\u001b[0m         buffer\u001b[39m.\u001b[39mbind()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introtoai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66df5dfc278ee59cd0d551fbefd7b63f907ce7594fd2436ca545d5a1f72fbc19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
